{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install huggingface_hub \n!pip install torch \n!pip install numpy \n!pip install fasttext \n!pip install pandas \n!pip install transformers \n!pip install scikit-learn \n!pip install sentence-transformers\n\n!pip install gdown","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T13:32:18.541609Z","iopub.execute_input":"2024-11-27T13:32:18.542262Z","iopub.status.idle":"2024-11-27T13:33:32.392093Z","shell.execute_reply.started":"2024-11-27T13:32:18.542226Z","shell.execute_reply":"2024-11-27T13:33:32.391013Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Download Dataset","metadata":{}},{"cell_type":"code","source":"!gdown --folder https://drive.google.com/drive/folders/15N-OFla0yl0wKxdnCs7MCdmXglTeST4I?usp=drive_link","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T13:00:29.672040Z","iopub.execute_input":"2024-11-20T13:00:29.672500Z","iopub.status.idle":"2024-11-20T13:00:39.411417Z","shell.execute_reply.started":"2024-11-20T13:00:29.672465Z","shell.execute_reply":"2024-11-20T13:00:39.410244Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Download TXT","metadata":{}},{"cell_type":"code","source":"!gdown '1DUxOSd4IJLdzBgS8vJSl1pPg0coGzCMh'\n!gdown '1F-6ckvKpYwUlqE5W-Ibdd3FmzOcnmN7d'\n!gdown \"1Nt3UQs3Y6Ip39irOl-d9MK2-h7K4L7AY\"\n!tar -xvf relative_init_vectors.tar.gz\n!tar -xvf ft_word_embedding.tar.gz","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T13:33:39.294180Z","iopub.execute_input":"2024-11-27T13:33:39.294547Z","iopub.status.idle":"2024-11-27T13:34:23.272160Z","shell.execute_reply.started":"2024-11-27T13:33:39.294516Z","shell.execute_reply":"2024-11-27T13:34:23.271033Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Function to reduce dataset size","metadata":{}},{"cell_type":"code","source":"# import random\n\n# def reduce(input_path, output_path, percentage=10):\n#     \"\"\"\n#     Reduce the size of the relation_init_vectors file by sampling a subset of lines.\n\n#     Args:\n#         input_path (str): Path to the original relation_init_vectors file.\n#         output_path (str): Path to save the reduced file.\n#         percentage (int): Percentage of lines to retain (excluding the header).\n#     \"\"\"\n#     with open(input_path, 'r', encoding='utf-8') as infile:\n#         lines = infile.readlines()\n\n#     print(f\"Original file {input_path} with {len(lines)} relations.\")\n\n#     # Keep the first line (header) intact\n#     header = lines[0]\n#     data_lines = lines[1:]\n\n#     # Calculate the number of lines to retain\n#     num_lines_to_keep = int(len(data_lines) * (percentage / 100))\n\n#     # Randomly sample the lines\n#     sampled_lines = random.sample(data_lines, num_lines_to_keep)\n\n#     # Combine the header and sampled lines\n#     reduced_lines = [header] + sampled_lines\n\n#     # Write the reduced lines to the output file\n#     with open(output_path, 'w', encoding='utf-8') as outfile:\n#         outfile.writelines(reduced_lines)\n\n#     print(f\"Reduced file saved to {output_path} with {len(sampled_lines)} relations.\")\n\n# reduce(\n#     input_path=\"relative_init_vectors.txt\",\n#     output_path=\"relative_init_vectors_small.txt\",\n#     percentage=1\n# )\n\n# reduce(\n#     input_path=\"ft_word_embeddings.txt\",\n#     output_path=\"ft_word_embeddings_small.txt\",\n#     percentage=1\n# )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T13:34:25.840995Z","iopub.execute_input":"2024-11-27T13:34:25.841390Z","iopub.status.idle":"2024-11-27T13:34:29.061245Z","shell.execute_reply.started":"2024-11-27T13:34:25.841358Z","shell.execute_reply":"2024-11-27T13:34:29.060296Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Download BGE Model","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import hf_hub_download\nfiles = [\n    \"1_Pooling/config.json\",\n    \"config_sentence_transformers.json\",\n    \"config.json\",\n    \"model.safetensors\",\n    \"modules.json\",\n    \"README.md\",\n    \"sentence_bert_config.json\",\n    \"special_tokens_map.json\",\n    \"tokenizer_config.json\",\n    \"tokenizer.json\",\n    \"vocab.txt\",\n]\nrepo_id = \"BAAI/bge-base-en-v1.5\"\nfor file in files:\n    hf_hub_download(repo_id=repo_id, filename=file, local_dir=\"./bge-base\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Download Model","metadata":{}},{"cell_type":"code","source":"import fasttext.util\nfasttext.util.download_model('en', if_exists='ignore')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## train-rwe.ipynb","metadata":{}},{"cell_type":"markdown","source":"### Libraries","metadata":{}},{"cell_type":"code","source":"import sys\nimport random\nimport torch\nimport numpy as np\nimport os\nos.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T13:34:33.603154Z","iopub.execute_input":"2024-11-27T13:34:33.603516Z","iopub.status.idle":"2024-11-27T13:34:36.459714Z","shell.execute_reply.started":"2024-11-27T13:34:33.603483Z","shell.execute_reply":"2024-11-27T13:34:36.459018Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Define Networks","metadata":{}},{"cell_type":"code","source":"# Define neural network model\nclass RWE_Model(torch.nn.Module):\n    def __init__(\n        self,\n        embedding_size_input,\n        embedding_size_output,\n        embedding_weights,\n        hidden_size,\n        dropout,\n    ):\n        super(RWE_Model, self).__init__()\n        self.embeddings = torch.nn.Embedding.from_pretrained(embedding_weights).float()\n        self.embeddings.weight.requires_grad = True\n        self.linear1 = torch.nn.Linear(embedding_size_input * 2, hidden_size)\n        self.relu = torch.nn.ReLU()\n        self.dropout = torch.nn.Dropout(dropout)\n        self.linear2 = torch.nn.Linear(hidden_size, embedding_size_output)\n\n    def forward(self, input1, input2):\n        embed1 = self.embeddings(input1)\n        embed2 = self.embeddings(input2)\n        out = self.linear1(\n            torch.cat(((embed1 * embed2), (embed1 + embed2) / 2), 2)\n        ).squeeze()\n        out = self.relu(out)\n        out = self.dropout(out)\n        out = self.linear2(out)\n        return out\n\n\n# Define function to get the model\ndef getRWEModel(\n    embedding_size_input, embedding_size_output, embedding_weights, hidden_size, dropout\n):\n    vocab_size = len(embedding_weights)\n    model = RWE_Model(\n        embedding_size_input,\n        embedding_size_output,\n        embedding_weights,\n        hidden_size,\n        dropout,\n    )\n    criterion = torch.nn.MSELoss()\n    return model.cuda(), criterion","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T13:34:47.149540Z","iopub.execute_input":"2024-11-27T13:34:47.150359Z","iopub.status.idle":"2024-11-27T13:34:47.157543Z","shell.execute_reply.started":"2024-11-27T13:34:47.150324Z","shell.execute_reply":"2024-11-27T13:34:47.156677Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Helper functions to train the model","metadata":{}},{"cell_type":"code","source":"# @title Helper functions to train the model\ndef load_vocab_embeddings(input_path):\n    first_line = True\n    vocab = set()\n    input_file_relations = open(input_file_relations, \"r\", encoding=\"utf-8\")\n    for line in input_file_relations:\n        if first_line == True:\n            first_line = False\n        else:\n            vocab.add(line.strip().split(\" \")[0])\n    return vocab\n\n\ndef load_word_vocab_from_relation_vectors(input_path):\n    pre_word_vocab = set()\n    first_line = True\n    input_file_relations = open(input_path, \"r\", encoding=\"utf-8\")\n    for line in input_file_relations:\n        linesplit = line.strip().split(\" \")\n        if first_line == True:\n            first_line = False\n        else:\n            relation = linesplit[0]\n            if \"__\" not in relation:\n                sys.exit(\"ERROR: Pair '\" + relation + \"' does not contain underscore\")\n            relation_split = relation.rsplit(\"__\", 1)\n            word1 = relation_split[0]\n            word2 = relation_split[1]\n            pre_word_vocab.add(word1)\n            pre_word_vocab.add(word2)\n    return pre_word_vocab\n\n\ndef load_embeddings_filtered_byvocab(input_path, vocab):\n    word2index = {}\n    index2word = {}\n    matrix_word_embeddings = []\n    first_line = True\n    input_file_relations = open(input_path, \"r\", encoding=\"utf-8\")\n    cont = 0\n    for line in input_file_relations:\n        linesplit = line.strip().split(\" \")\n        if first_line == True:\n            dimensions = int(linesplit[1])\n            first_line = False\n        else:\n            word = linesplit[0]\n            if word in vocab and word not in word2index:\n                word2index[word] = cont\n                index2word[cont] = word\n                cont += 1\n                matrix_word_embeddings.append(\n                    np.asarray([float(dim) for dim in linesplit[1 : dimensions + 1]])\n                )\n    return matrix_word_embeddings, word2index, index2word, dimensions\n\n\ndef load_training_data(input_path, matrix_word_embeddings, word2index):\n    matrix_input = []\n    matrix_output = []\n    first_line = True\n    input_file_relations = open(input_path, \"r\", encoding=\"utf-8\")\n    for line in input_file_relations:\n        linesplit = line.strip().split(\" \")\n        if first_line == True:\n            dimensions = int(str(line.split(\" \")[1]))\n            first_line = False\n        else:\n            relation = linesplit[0]\n            if \"__\" not in relation:\n                sys.exit(\"ERROR: Pair '\" + relation + \"' does not contain underscore\")\n            relation_split = relation.rsplit(\"__\", 1)\n            word1 = relation_split[0]\n            word2 = relation_split[1]\n            if word1 in word2index and word2 in word2index:\n                matrix_input.append(np.asarray([word2index[word1], word2index[word2]]))\n                matrix_output.append(\n                    np.asarray([float(dim) for dim in linesplit[1 : dimensions + 1]])\n                )\n    return matrix_input, matrix_output, dimensions\n\n\ndef split_training_data(matrix_input, matrix_output, devsize, batchsize):\n    matrix_input_train = []\n    matrix_output_train = []\n    matrix_input_dev = []\n    matrix_output_dev = []\n    num_instances = int((len(matrix_input) // batchsize) * batchsize)\n    final_size_dev = int(((num_instances * devsize) // batchsize) * batchsize)\n    final_size_train = int(((num_instances - final_size_dev) // batchsize) * batchsize)\n    print(\"Size train set: \" + str(final_size_train))\n    print(\"Size dev set: \" + str(final_size_dev))\n    all_instances = range(num_instances)\n    list_index_dev = random.sample(all_instances, final_size_dev)\n    for i in range(num_instances):\n        if i in list_index_dev:\n            matrix_input_dev.append(matrix_input[i])\n            matrix_output_dev.append(matrix_output[i])\n        else:\n            matrix_input_train.append(matrix_input[i])\n            matrix_output_train.append(matrix_output[i])\n    return matrix_input_train, matrix_output_train, matrix_input_dev, matrix_output_dev\n\n\ndef trainIntervals(model, optimizer, criterion, batches, interval=100, lr=0.1):\n    i = 0\n    n = 0\n    trainErr = 0\n    for x1, x2, y in zip(*batches):\n        model.train()\n        optimizer.zero_grad()\n        trainErr += gradUpdate(model, x1, x2, y, criterion, optimizer, lr)\n        i += 1\n        if i == interval:\n            n += 1\n            prev_train_err = trainErr\n            trainErr = 0\n            i = 0\n    if i > 0 and prev_train_err != 0:\n        print(\"Training error: \" + str(prev_train_err / float(i)))\n\n\ndef validate(model, batches, criterion):\n    evalErr = 0\n    n = 0\n    model.eval()\n    for x1, x2, y in zip(*batches):\n        y = torch.autograd.Variable(y, requires_grad=False)\n        x1 = torch.autograd.Variable(x1, requires_grad=False)\n        x2 = torch.autograd.Variable(x2, requires_grad=False)\n        output = model(x1, x2)\n        error = criterion(output, y)\n        evalErr += error.item()\n        n += 1\n    return evalErr / n\n\n\ndef gradUpdate(model, x1, x2, y, criterion, optimizer, lr):\n    output = model(x1, x2)\n    error = criterion(output, y)\n    error.backward()\n    optimizer.step()\n    return error.item()\n\n\ndef getBatches(data, batchSize):\n    embsize = int(data.size(-1))\n    return data.view(-1, batchSize, embsize)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T13:34:49.293652Z","iopub.execute_input":"2024-11-27T13:34:49.294019Z","iopub.status.idle":"2024-11-27T13:34:49.313439Z","shell.execute_reply.started":"2024-11-27T13:34:49.293988Z","shell.execute_reply":"2024-11-27T13:34:49.312616Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define function to train the model\ndef trainEpochs(\n    model,\n    optimizer,\n    criterion,\n    trainBatches,\n    validBatches,\n    epochs=10,\n    interval=100,\n    lr=0.1,\n):\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, patience=2, threshold=1e-7, factor=0.9\n    )\n    min_error = -1.0\n    for epoch in range(1, epochs + 1):\n        print(\"\\n     ----------    \\n\")\n        print(\"EPOCH \" + str(epoch))\n        print(\"Starting training epoch \" + str(epoch))\n        trainIntervals(model, optimizer, criterion, trainBatches, interval, lr)\n        validErr = validate(model, validBatches, criterion)\n        scheduler.step(validErr)\n        print(\"Validation error : \" + str(validErr))\n        if validErr < min_error or min_error == -1.0:\n            new_model = model\n            min_error = validErr\n            print(\n                \"[Model at epoch \"\n                + str(epoch)\n                + \" obtained the lowest development error rate so far.]\"\n            )\n        # if epoch % 5 == 0 or epoch == 1: torch.save(model, f\"./model-epoch{str(epoch)}.model\")\n        torch.save(model, f\"epoch-{epoch}.model\")\n        print(\"Epoch \" + str(epoch) + \" done\")\n    return new_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T13:34:52.392821Z","iopub.execute_input":"2024-11-27T13:34:52.393527Z","iopub.status.idle":"2024-11-27T13:34:52.399779Z","shell.execute_reply.started":"2024-11-27T13:34:52.393483Z","shell.execute_reply":"2024-11-27T13:34:52.398934Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Define driver function to actually load the data for the model training","metadata":{}},{"cell_type":"code","source":"# @title Define driver function to actually load the data for the model training\ndef train_rwe(\n    word_embeddings_path,\n    rel_embeddings_path,\n    output_path,\n    hidden_size=0,\n    dropout=0.5,\n    epochs=5,\n    interval=100,\n    batchsize=10,\n    dev_size=0.015,\n    lr=0.01,\n):\n    if dev_size >= 1 or dev_size < 0:\n        raise Exception(\n            \"Development data should be between 0% (0.0) and 100% (1.0) of the training data\"\n        )\n\n    print(\"Loading word vocabulary...\")\n    pre_word_vocab = load_word_vocab_from_relation_vectors(rel_embeddings_path)\n\n    # count = 0\n\n    # for word in pre_word_vocab:\n    #     if count < 10:\n    #         print(word)\n    #     count+=1\n    \n    print(\n        \"Word vocabulary loaded succesfully (\"\n        + str(len(pre_word_vocab))\n        + \" words). Now loading word embeddings...\"\n    )\n    (\n        matrix_word_embeddings,\n        word2index,\n        index2word,\n        dims_word,\n    ) = load_embeddings_filtered_byvocab(word_embeddings_path, pre_word_vocab)\n\n\n    # count = 0\n\n    # for matrix in matrix_word_embeddings:\n    #     if count < 1:\n    #         print(matrix)\n    #     count+=1\n    \n    pre_word_vocab.clear()\n    print(\n        \"Word embeddings loaded succesfully (\"\n        + str(dims_word)\n        + \" dimensions). Now loading relation vectors...\"\n    )\n    matrix_input, matrix_output, dims_rels = load_training_data(\n        rel_embeddings_path, matrix_word_embeddings, word2index\n    )\n\n    # count = 0\n\n    # for matrix in matrix_input:\n    #     if count < 1:\n    #         print(matrix)\n    #     count+=1\n    \n\n    # count = 0\n\n    # for matrix in matrix_output:\n    #     if count < 1:\n    #         print(matrix)\n    #     count+=1\n    \n    print(\n        \"Relation vectors loaded (\"\n        + str(dims_rels)\n        + \" dimensions), now spliting training and dev...\"\n    )\n    random.seed(21)\n    s1 = random.getstate()\n    random.shuffle(matrix_input)\n    random.setstate(s1)\n    random.shuffle(matrix_output)\n    (\n        matrix_input_train,\n        matrix_output_train,\n        matrix_input_dev,\n        matrix_output_dev,\n    ) = split_training_data(matrix_input, matrix_output, dev_size, batchsize)\n    matrix_input.clear()\n    matrix_output.clear()\n    print(\"Done preprocessing all the data, now loading and training the model...\\n\")\n\n    if hidden_size == 0:\n        hidden_size = dims_word * 2\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(\"Device used: \" + str(device))\n    embedding_weights = torch.tensor(matrix_word_embeddings)\n    matrix_word_embeddings.clear()\n    tensor_input_train_1 = torch.LongTensor([[x[0]] for x in matrix_input_train])\n    tensor_input_train_2 = torch.LongTensor([[x[1]] for x in matrix_input_train])\n    matrix_input_train.clear()\n    tensor_input_dev_1 = torch.LongTensor([[x[0]] for x in matrix_input_dev])\n    tensor_input_dev_2 = torch.LongTensor([[x[1]] for x in matrix_input_dev])\n    matrix_input_dev.clear()\n    tensor_output_train = torch.FloatTensor(matrix_output_train)\n    matrix_output_train.clear()\n    tensor_output_dev = torch.FloatTensor(matrix_output_dev)\n    matrix_output_dev.clear()\n    model, criterion = getRWEModel(\n        dims_word, dims_rels, embedding_weights, hidden_size, dropout\n    )\n    print(\"RWE model loaded.\")\n    optimizer = torch.optim.Adam(model.parameters(), lr)\n    trainX1batches = getBatches(tensor_input_train_1.cuda(), batchsize)\n    trainX2batches = getBatches(tensor_input_train_2.cuda(), batchsize)\n    validX1Batches = getBatches(tensor_input_dev_1.cuda(), batchsize)\n    validX2Batches = getBatches(tensor_input_dev_2.cuda(), batchsize)\n    trainYBatches = getBatches(tensor_output_train.cuda(), batchsize)\n    validYBatches = getBatches(tensor_output_dev.cuda(), batchsize)\n    print(\"Now starting training...\\n\")\n    output_model = trainEpochs(\n        model,\n        optimizer,\n        criterion,\n        (trainX1batches, trainX2batches, trainYBatches),\n        (validX1Batches, validX2Batches, validYBatches),\n        epochs,\n        interval,\n        lr,\n    )\n    print(\n        \"\\nTraining finished. Now loading relational word embeddings from trained model...\"\n    )\n\n    parameters = list(output_model.parameters())\n    num_vectors = len(parameters[0])\n    print(\"Number of vectors: \" + str(num_vectors))\n    num_dimensions = len(parameters[0][0])\n    print(\"Number of dimensions output embeddings: \" + str(num_dimensions))\n    txtfile = open(output_path, \"w\", encoding=\"utf8\")\n    txtfile.write(str(num_vectors) + \" \" + str(num_dimensions) + \"\\n\")\n    if num_vectors != embedding_weights.size()[0]:\n        print(\n            \"Something is wrong in the input vectors: \"\n            + str(embedding_weights.size()[0])\n            + \" != \"\n            + str(num_vectors)\n        )\n\n    # count = 0\n    for i in range(num_vectors):\n        word = index2word[i]\n        txtfile.write(word)\n        vector = parameters[0][i].cpu().detach().numpy()\n        # if count < 4:\n        #     print(word, vector)\n        #     count += 1\n        for dimension in vector:\n            txtfile.write(\" \" + str(dimension))\n        txtfile.write(\"\\n\")\n    txtfile.close()\n    print(\"\\nFINISHED. Word embeddings stored at \" + output_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T13:34:55.409092Z","iopub.execute_input":"2024-11-27T13:34:55.409689Z","iopub.status.idle":"2024-11-27T13:34:55.426149Z","shell.execute_reply.started":"2024-11-27T13:34:55.409635Z","shell.execute_reply":"2024-11-27T13:34:55.425191Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Train Model","metadata":{}},{"cell_type":"code","source":"# train the model\ntrain_rwe(\n    \"/kaggle/working/ft_word_embeddings.txt\",\n    \"/kaggle/working/relative_init_vectors.txt\",\n    \"/kaggle/working/rwe_embeddings.txt\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T13:34:58.662121Z","iopub.execute_input":"2024-11-27T13:34:58.662452Z","iopub.status.idle":"2024-11-27T13:35:06.483667Z","shell.execute_reply.started":"2024-11-27T13:34:58.662426Z","shell.execute_reply":"2024-11-27T13:35:06.482761Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Classification.ipynb","metadata":{}},{"cell_type":"code","source":"import io\nimport os\nimport fasttext\nimport pandas as pd\nimport numpy as np\n\nfrom transformers import set_seed\nfrom sklearn.neural_network import MLPClassifier\nfrom sentence_transformers.SentenceTransformer import SentenceTransformer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T12:58:31.283799Z","iopub.execute_input":"2024-11-20T12:58:31.284580Z","iopub.status.idle":"2024-11-20T12:58:47.334947Z","shell.execute_reply.started":"2024-11-20T12:58:31.284544Z","shell.execute_reply":"2024-11-20T12:58:47.333948Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\nset_seed(42, deterministic=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train = pd.read_csv(\"dataset/train.tsv\", sep=\"\\t\", header=None, names=[\"kata1\",\"kata2\",\"relasi\"]).dropna().reset_index(drop=True)\ntest = pd.read_csv(\"dataset/test.tsv\", sep=\"\\t\", header=None, names=[\"kata1\",\"kata2\",\"relasi\"]).dropna().reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T13:01:01.629897Z","iopub.execute_input":"2024-11-20T13:01:01.630772Z","iopub.status.idle":"2024-11-20T13:01:01.658606Z","shell.execute_reply.started":"2024-11-20T13:01:01.630734Z","shell.execute_reply":"2024-11-20T13:01:01.657759Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Check class distribution for the training dataset\n# print(\"Class Distribution in Training Dataset:\")\n# train_class_distribution = train['relasi'].value_counts()\n# print(train_class_distribution)\n\n# # Check class distribution for the testing dataset\n# print(\"\\nClass Distribution in Testing Dataset:\")\n# test_class_distribution = test['relasi'].value_counts()\n# print(test_class_distribution)\n\n# # Check if the distribution is balanced\n# def is_balanced(distribution):\n#     counts = distribution.values\n#     return max(counts) - min(counts) <= (sum(counts) * 0.05)  # Allow 5% tolerance\n\n# print(\"\\nIs the training dataset balanced?\", is_balanced(train_class_distribution))\n# print(\"Is the testing dataset balanced?\", is_balanced(test_class_distribution))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T13:01:09.753675Z","iopub.execute_input":"2024-11-20T13:01:09.754011Z","iopub.status.idle":"2024-11-20T13:01:09.774666Z","shell.execute_reply.started":"2024-11-20T13:01:09.753981Z","shell.execute_reply":"2024-11-20T13:01:09.773634Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_learned_embeddings(name: str):\n    fin = io.open(name, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n    data = {}\n    for line in fin:\n        tokens = line.rstrip().split(' ')\n        data[tokens[0]] = np.array(tokens[1:], dtype=float)\n    return data\n# rwe = load_learned_embeddings('rwe_embeddings.txt')  # Self-trained\nrwe = load_learned_embeddings('reference_rwe.txt')  # From original repo\nft = fasttext.load_model('cc.en.300.bin')\nembedding_model = SentenceTransformer('bge-base', local_files_only=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Training","metadata":{}},{"cell_type":"code","source":"def get_bge_train_data(add_rwe=False):\n    representations = []\n    word1s_embeddings = embedding_model.encode(\n        train[\"kata1\"].to_list(), batch_size=256, show_progress_bar=True\n    )\n    word2s_embeddings = embedding_model.encode(\n        train[\"kata2\"].to_list(), batch_size=256, show_progress_bar=True\n    )\n\n    for i in range(len(train)):\n        word1_embed = word1s_embeddings[i]\n        word2_embed = word2s_embeddings[i]\n\n        if add_rwe:\n            word1_rwe = rwe.get(train[\"kata1\"][i], np.zeros(300,))\n            word2_rwe = rwe.get(train[\"kata2\"][i], np.zeros(300,))\n            word1_embed = np.concatenate((word1_embed, word1_rwe))\n            word2_embed = np.concatenate((word2_embed, word2_rwe))\n\n        pair_difference = np.subtract(word1_embed, word2_embed)\n        representations.append(pair_difference)\n\n    train[\"representation\"] = representations\n    X_train = np.vstack(train[\"representation\"])\n    y_train = train[\"relasi\"]\n\n    return X_train, y_train\n\ndef get_fasttext_train_data(add_rwe=False):\n    representations = []\n    for i in range(len(train)):\n        word1_embed = ft.get_word_vector(train[\"kata1\"][i])\n        word2_embed = ft.get_word_vector(train[\"kata2\"][i])\n\n        if add_rwe:\n            word1_rwe = rwe.get(train[\"kata1\"][i], np.zeros(300,))\n            word2_rwe = rwe.get(train[\"kata2\"][i], np.zeros(300,))\n            word1_embed = np.concatenate((word1_embed, word1_rwe))\n            word2_embed = np.concatenate((word2_embed, word2_rwe))\n        \n        pair_difference = np.subtract(word1_embed, word2_embed)\n        representations.append(pair_difference)\n\n    train['representation'] = representations\n    X_train = np.vstack(train['representation'])\n    y_train = train['relasi']\n\n    return X_train, y_train","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train_bge, y_train_bge = get_bge_train_data()\nX_train_bge_rwe, y_train_bge_rwe = get_bge_train_data(True)\nX_train_ft, y_train_ft = get_fasttext_train_data()\nX_train_ft_rwe, y_train_ft_rwe = get_fasttext_train_data(True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_bge = MLPClassifier(random_state=42, max_iter=300)\nmodel_bge_rwe = MLPClassifier(random_state=42, max_iter=300)\nmodel_ft = MLPClassifier(random_state=42, max_iter=300)\nmodel_ft_rwe = MLPClassifier(random_state=42, max_iter=300)\n\nmodel_bge.fit(X_train_bge, y_train_bge)\nmodel_bge_rwe.fit(X_train_bge_rwe, y_train_bge_rwe)\nmodel_ft.fit(X_train_ft, y_train_ft)\nmodel_ft_rwe.fit(X_train_ft_rwe, y_train_ft_rwe)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Testing","metadata":{}},{"cell_type":"code","source":"def get_bge_test_data(add_rwe=False):\n    representations = []\n    word1s_embeddings = embedding_model.encode(\n        test[\"kata1\"].to_list(), batch_size=256, show_progress_bar=True\n    )\n    word2s_embeddings = embedding_model.encode(\n        test[\"kata2\"].to_list(), batch_size=256, show_progress_bar=True\n    )\n\n    for i in range(len(test)):\n        word1_embed = word1s_embeddings[i]\n        word2_embed = word2s_embeddings[i]\n\n        if add_rwe:\n            word1_rwe = rwe.get(test[\"kata1\"][i], np.zeros(300,))\n            word2_rwe = rwe.get(test[\"kata2\"][i], np.zeros(300,))\n            word1_embed = np.concatenate((word1_embed, word1_rwe))\n            word2_embed = np.concatenate((word2_embed, word2_rwe))\n\n        pair_difference = np.subtract(word1_embed, word2_embed)\n        representations.append(pair_difference)\n\n    test[\"representation\"] = representations\n    X_test = np.vstack(test[\"representation\"])\n    y_test = test[\"relasi\"]\n\n    return X_test, y_test\n\ndef get_fasttext_test_data(add_rwe=False):\n    representations = []\n    for i in range(len(test)):\n        word1_embed = ft.get_word_vector(test[\"kata1\"][i])\n        word2_embed = ft.get_word_vector(test[\"kata2\"][i])\n\n        if add_rwe:\n            word1_rwe = rwe.get(test[\"kata1\"][i], np.zeros(300,))\n            word2_rwe = rwe.get(test[\"kata2\"][i], np.zeros(300,))\n            word1_embed = np.concatenate((word1_embed, word1_rwe))\n            word2_embed = np.concatenate((word2_embed, word2_rwe))\n        \n        pair_difference = np.subtract(word1_embed, word2_embed)\n        representations.append(pair_difference)\n\n    test['representation'] = representations\n    X_test = np.vstack(test['representation'])\n    y_test = test['relasi']\n\n    return X_test, y_test\n\ndef get_accuracy(pred, actual):\n    sum = 0\n    for i in range(len(pred)):\n        if pred[i] == actual[i]:\n            sum += 1\n    print(f\"Accuracy: {sum/len(pred)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_test_bge, y_test_bge = get_bge_test_data()\nX_test_bge_rwe, y_test_bge_rwe = get_bge_test_data(True)\nX_test_ft, y_test_ft = get_fasttext_test_data()\nX_test_ft_rwe, y_test_ft_rwe = get_fasttext_test_data(True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prediction_bge = model_bge.predict(X_test_bge)\nprediction_bge_rwe = model_bge_rwe.predict(X_test_bge_rwe)\nprediction_ft = model_ft.predict(X_test_ft)\nprediction_ft_rwe = model_ft_rwe.predict(X_test_ft_rwe)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"get_accuracy(prediction_bge, y_test_bge)\nget_accuracy(prediction_bge_rwe, y_test_bge_rwe)\nget_accuracy(prediction_ft, y_test_ft)\nget_accuracy(prediction_ft_rwe, y_test_ft_rwe)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Display Transformation","metadata":{}},{"cell_type":"code","source":"\ndef display_transformation(X_test, y_test, predictions, test_data, limit=5):\n    print(f\"{'Kata1':<15} {'Kata2':<15} {'Relasi':<10} {'Predicted':<10}\")\n    print(\"-\" * 50)\n    \n    # Loop through a limited number of examples\n    for i in range(min(limit, len(test_data))):\n        kata1 = test_data[\"kata1\"].iloc[i]\n        kata2 = test_data[\"kata2\"].iloc[i]\n        relasi = y_test[i]\n        predicted = predictions[i]\n        print(f\"{kata1:<15} {kata2:<15} {relasi:<10} {predicted:<10}\")\n\n# Display transformations for BGE without RWE\nprint(\"BGE without RWE:\")\ndisplay_transformation(X_test_bge, y_test_bge, prediction_bge, test, limit=5)\n\n# Display transformations for BGE with RWE\nprint(\"\\nBGE with RWE:\")\ndisplay_transformation(X_test_bge_rwe, y_test_bge_rwe, prediction_bge_rwe, test, limit=5)\n\n# Display transformations for FastText without RWE\nprint(\"\\nFastText without RWE:\")\ndisplay_transformation(X_test_ft, y_test_ft, prediction_ft, test, limit=5)\n\n# Display transformations for FastText with RWE\nprint(\"\\nFastText with RWE:\")\ndisplay_transformation(X_test_ft_rwe, y_test_ft_rwe, prediction_ft_rwe, test, limit=5)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}